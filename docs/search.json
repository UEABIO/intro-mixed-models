[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"Data often complex messy.can different grouping factors like collect data, etc. Sample sizes may also leave something desired, especially try fit complicated models many parameters. top , data points might truly independent. instance, might structure data.mixed models developed, deal messy data allow us use data, even low sample sizes, structured data many covariates fit. top , mixed models allow us save degrees freedom compared running standard linear models! Sounds good.cover basics linear mixed models, use responsibly interepret findings effectively.trying “extend” linear model, fear : generalised linear mixed effects models, .","code":""},{"path":"index.html","id":"terminology","chapter":"Overview","heading":"0.1 Terminology","text":"Mixed-effects models sometime referred hierarchical models, multi-level models, random effects models, mixed-models. Regardless confusing vocabulary, ’s worth knowing terms may used mean similar models.Random effects important part mixed-effects models. Random effects capture variations come grouping clustering data. used mixed models, combine fixed effects (relationships variables) random effects. approach helps us account correlations dependencies within groups, making models realistic.nature, often see hierarchical structures, streams within watershed species within family. Random effects can useful situations others observations tend clustered. using random effects, can improve ability model system accurately.main purpose applying random effects mixed models capture realistic patterns uncertainties data. example, can account correlations arise multiple observations within group observations lack complete independence.sum , random effects mixed models help us deal situations observations independent. allow us capture complexities dependencies within groups, improving accuracy statistical models.","code":""},{"path":"index.html","id":"variance","chapter":"Overview","heading":"0.1.1 Variance","text":"understand mixed- random effects models, need understand random effects understand random effects, need understand variance.Variance statistical measure quantifies spread dispersion set data points. provides measure much values dataset deviate mean.calculate variance linear model (ordinary least squares), first obtain residuals, differences observed response values predicted values. , square residual eliminate negative signs calculate sum squared residuals. Finally, divide sum degrees freedom, total number observations minus number estimated coefficients model.o estimate variance components linear mixed model, likelihood-based methods maximum likelihood estimation (MLE) restricted maximum likelihood estimation (REML) commonly used. methods optimize likelihood function adjusting model parameters, including variance components, find values best fit observed data.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"foundations-of-mixed-modelling","chapter":"1 Foundations of Mixed Modelling","heading":"1 Foundations of Mixed Modelling","text":"","code":""},{"path":"foundations-of-mixed-modelling.html","id":"what-is-a-mixed-model","chapter":"1 Foundations of Mixed Modelling","heading":"1.1 What is a mixed model?","text":"Mixed models (also known linear mixed models hierarchical linear models) statistical tests build simpler tests regression, t-tests ANOVA. tests special cases general linear model; fit straight line data explain variance systematic way.key difference linear mixed-effects model inclusion random effects - variables observations grouped subcategories systematically affect outcome - account important structure data.mixed-effects model can used many situations instead one straightforward tests structure may important. main advantages approach :mixed-effects models account variancemixed-effects models incorporate group even individual-level differencesmixed-effects models cope well missing data, unequal group sizes repeated measurements","code":""},{"path":"foundations-of-mixed-modelling.html","id":"fixed-vs-random-effects","chapter":"1 Foundations of Mixed Modelling","heading":"1.2 Fixed vs Random effects","text":"Fixed effects random effects terms commonly used mixed modeling, statistical framework combines order analyze data.mixed modeling, fixed effects used estimate overall relationship predictors response variable, random effects account within-group variability allow modeling individual differences group-specific effects.hierarchical structure data refers data organization observations nested within higher-level groups clusters. example, students nested within classrooms, patients nested within hospitals, employees nested within companies. hierarchical structure introduces dependencies correlations within data, observations within group tend similar observations different groups.need mixed models arises want account dependencies properly model variability different levels hierarchy. Traditional regression models, ordinary least squares (OLS), assume observations independent . However, working hierarchical data, assumption violated, ignoring hierarchical structure can lead biased inefficient estimates, incorrect standard errors, misleading inference.including random effects, mixed models allow estimation within-group -group variability. provide flexible framework modeling individual group-specific effects can capture heterogeneity within groups. Additionally, mixed models can handle unbalanced incomplete data, groups may different numbers observations.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"fixed-effects","chapter":"1 Foundations of Mixed Modelling","heading":"1.2.1 Fixed effects","text":"broad terms, fixed effects variables expect affect dependent/response variable: ’re call explanatory variables standard linear regression.Fixed effects common random effects, least use. Fixed effects estimate different levels relationship assumed levels. example, model dependent variable body length fixed effect fish sex, get estimate mean body length males estimate females separately.can consider terms simple linear model, estimated intercept expected value outcome \\(y\\) predictor \\(x\\) value 0. estimated slope expected change \\(y\\) single unit change \\(x\\). parameters \"fixed\", meaning individual population expected value intercept slope.difference expected value true value called \"residual error\".\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]","code":""},{"path":"foundations-of-mixed-modelling.html","id":"examples","chapter":"1 Foundations of Mixed Modelling","heading":"1.2.1.1 Examples:","text":"Medical Research: clinical trial studying effectiveness different medications treating specific condition, fixed effects include categorical variables treatment group (e.g., medication , medication B, placebo) dosage level (e.g., low, medium, high). fixed effects capture systematic differences response variable (e.g., symptom improvement) due specific treatment received.Medical Research: clinical trial studying effectiveness different medications treating specific condition, fixed effects include categorical variables treatment group (e.g., medication , medication B, placebo) dosage level (e.g., low, medium, high). fixed effects capture systematic differences response variable (e.g., symptom improvement) due specific treatment received.Education Research: Suppose study examines impact teaching methods student performance different schools. fixed effects case might include variables school type (e.g., public, private), curriculum approach (e.g., traditional, progressive), classroom size. fixed effects help explain differences student achievement across schools, accounting systematic effects factors.Education Research: Suppose study examines impact teaching methods student performance different schools. fixed effects case might include variables school type (e.g., public, private), curriculum approach (e.g., traditional, progressive), classroom size. fixed effects help explain differences student achievement across schools, accounting systematic effects factors.Environmental Science: Imagine study investigating factors influencing bird species richness across different habitats. fixed effects context include variables habitat type (e.g., forest, grassland, wetland), habitat disturbance level (e.g., low, medium, high), geographical region. fixed effects capture systematic variations bird species richness associated specific habitat characteristics.Environmental Science: Imagine study investigating factors influencing bird species richness across different habitats. fixed effects context include variables habitat type (e.g., forest, grassland, wetland), habitat disturbance level (e.g., low, medium, high), geographical region. fixed effects capture systematic variations bird species richness associated specific habitat characteristics.Fixed effects default effects learn begin understand statistical concepts, fixed effects default effects functions like lm() aov().","code":""},{"path":"foundations-of-mixed-modelling.html","id":"random-effects","chapter":"1 Foundations of Mixed Modelling","heading":"1.2.2 Random effects","text":"Random effects less commonly used perhaps widely encountered nature. level can considered random variable underlying process distribution random effect.random effect parameter allowed vary across groups individuals. Random effects take single fixed value, rather follow distribution (usually normal distribution). Random effects can added model account variation around intercept slope. individual group gets estimated random effect, representing adjustment mean.random effects usually grouping factors trying control. always categorical, can’t force R treat continuous variable random effect. lot time specifically interested impact response variable, know might influencing patterns see.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"examples-1","chapter":"1 Foundations of Mixed Modelling","heading":"1.2.2.1 Examples:","text":"Longitudinal Health Study: Consider study tracking blood pressure individuals multiple time points. case, random effect can included account individual-specific variation blood pressure. individual's blood pressure measurements time treated repeated measures within individual, random effect capture variability individuals explained fixed effects. random effect allows modeling inherent individual differences blood pressure levels.Longitudinal Health Study: Consider study tracking blood pressure individuals multiple time points. case, random effect can included account individual-specific variation blood pressure. individual's blood pressure measurements time treated repeated measures within individual, random effect capture variability individuals explained fixed effects. random effect allows modeling inherent individual differences blood pressure levels.Social Network Analysis: Suppose study examines influence peer groups adolescent behavior. study may collect data individual behaviors within schools, students nested within classrooms. scenario, random effect can incorporated classroom level account shared social environment within classroom. random effect captures variability behavior among classrooms accounted fixed effects, enabling study analyze effects individual-level classroom-level factors simultaneously.Social Network Analysis: Suppose study examines influence peer groups adolescent behavior. study may collect data individual behaviors within schools, students nested within classrooms. scenario, random effect can incorporated classroom level account shared social environment within classroom. random effect captures variability behavior among classrooms accounted fixed effects, enabling study analyze effects individual-level classroom-level factors simultaneously.Ecological Study: Imagine research project investigating effect environmental factors species abundance different study sites. study sites may geographically dispersed, random effect can included account variation study sites. random effect captures unexplained heterogeneity species abundance across different sites, allowing examination effects environmental variables accounting site-specific differences.Ecological Study: Imagine research project investigating effect environmental factors species abundance different study sites. study sites may geographically dispersed, random effect can included account variation study sites. random effect captures unexplained heterogeneity species abundance across different sites, allowing examination effects environmental variables accounting site-specific differences.random effect \\(group_i\\) often assumed follow normal distribution mean zero variance estimated model fitting process.\\[Y_i = β_0 + U_j + ε_i\\]\nbook “Data analysis using regression \nmultilevel/hierarchical models” (Gelman & Hill (2006)). authors examined five\ndefinitions fixed random effects found consistent\nagreement.\n\n\nFixed effects constant across individuals, random effects\nvary. example, growth study, model random intercepts a_i\nfixed slope b corresponds parallel lines different\nindividuals , model y_it = a_i + b t thus distinguish \nfixed random coefficients.\n\n\nFixed effects constant across individuals, random effects\nvary. example, growth study, model random intercepts a_i\nfixed slope b corresponds parallel lines different\nindividuals , model y_it = a_i + b t thus distinguish \nfixed random coefficients.\n\n\nEffects fixed interesting random\ninterest underlying population.\n\n\nEffects fixed interesting random\ninterest underlying population.\n\n\n“sample exhausts population, corresponding\nvariable fixed; sample small (.e., negligible) part \npopulation corresponding variable random.”\n\n\n“sample exhausts population, corresponding\nvariable fixed; sample small (.e., negligible) part \npopulation corresponding variable random.”\n\n\n“effect assumed realized value random\nvariable, called random effect.”\n\n\n“effect assumed realized value random\nvariable, called random effect.”\n\n\nFixed effects estimated using least squares (, \ngenerally, maximum likelihood) random effects estimated \nshrinkage.\n\n\nFixed effects estimated using least squares (, \ngenerally, maximum likelihood) random effects estimated \nshrinkage.\n\nThus turns fixed random effects born made. \nmust make decision treat variable fixed random \nparticular analysis.\ndetermining wht fixed random effect study, consider trying ? trying make predictions ? just variation (.k.“noise”) need control ?\nrandom effects:\n\nNote golden rule generally want random\neffect least five levels. , instance, wanted \ncontrol effects fish sex body length, fit sex (\ntwo level factor: male female) fixed, random, effect.\n\n, put simply, estimating variance data points\nimprecise. Mathematically , wouldn’t lot\nconfidence . two three levels, model\nstruggle partition variance - give output,\nnecessarily one can trust.\n\nFinally, keep mind name random doesn’t much \nmathematical randomness. Yes, ’s confusing. Just think \ngrouping variables now. Strictly speaking ’s \nmaking models representative questions getting better\nestimates. Hopefully, next examples help make sense \n’re used.\n\n’s firm rule minimum number factor levels\nrandom effect really can use factor \ntwo levels. However commonly reported may want\nfive factor levels random effect order really\nbenefit random effect can (though argue even\n, 10 levels). Another case may want random\neffect don’t want factor levels inform \ndon’t assume factor levels come common\ndistribution. noted , male female factor \ntwo levels oftentimes want male female information\nestimated separately ’re necessarily assuming males \nfemales come population sexes infinite\nnumber ’re interested average.\n","code":""},{"path":"foundations-of-mixed-modelling.html","id":"why-use-mixed-models","chapter":"1 Foundations of Mixed Modelling","heading":"1.3 Why use mixed models?","text":"provided code generates dataset suitable testing mixed models. break code annotate step:section creates data frame called rand_eff containing random effects. consists five levels grouping variable (group), level, generates random effects (b0 b1) using rnorm function.section creates main dataset (data) testing mixed models. uses expand.grid create combination levels grouping variable (group) observation variable (obs). performs left join rand_eff data frame, matching group variable incorporate random effects group.code continues mutate dataset adding additional variables:x random predictor variable generated using runif values 0 10.x random predictor variable generated using runif values 0 10.B0 B1 represent fixed effects intercept slope predetermined values 20 2, respectively.B0 B1 represent fixed effects intercept slope predetermined values 20 2, respectively.E represents error term, generated using rnorm mean 0 standard deviation 10.E represents error term, generated using rnorm mean 0 standard deviation 10.Finally, y created response variable using linear model equation includes fixed effects (B0 B1), random effects (b0 b1), predictor variable (x), error term (E).Finally, y created response variable using linear model equation includes fixed effects (B0 B1), random effects (b0 b1), predictor variable (x), error term (E).section creates additional dataset (data.1) specific group (group = 5) smaller number observations (obs = 30) testing purposes. appended original dataset, see effect smaller group within random effects discuss partial pooling shrinkage later .Now three variables consider models: x, y group (five levels).Now suitable simulated dataset, start modelling!","code":"\ndata <- expand.grid(group = as.factor(seq(1:10)), \n                    obs = as.factor(seq(1:100))) %>%\n  left_join(rand_eff,\n            by = \"group\") %>%\n  mutate(x = runif(n = nrow(.), 0, 10),\n         B0 = 20,\n         B1 = 2,\n         E = rnorm(n = nrow(.), 0, 10)) %>%\n  mutate(y = B0 + b0 + x * (B1 + b1) + E)\n\ndata <- expand.grid(group = as.factor(seq(1:4)), \n                    obs = as.factor(seq(1:100)))\ndata.1 <- expand.grid(group = as.factor(5),\n          obs = as.factor(seq(1:30)))\n\ndata <- bind_rows(data, data.1) %>% \n  left_join(rand_eff,\n            by = \"group\") %>%\n  mutate(x = runif(n = nrow(.), 0, 10),\n         B0 = 20,\n         B1 = 2,\n         E = rnorm(n = nrow(.), 0, 10)) %>%\n  mutate(y = B0 + b0 + x * (B1 + b1) + E)\ndata %>% \n  select(x, y, group, obs) %>% \n  head()"},{"path":"foundations-of-mixed-modelling.html","id":"all-in-one-model","chapter":"1 Foundations of Mixed Modelling","heading":"1.3.1 All in one model","text":"begin highlighting importance considering data structure hierarchy building linear models. illustrate , delve example showcases consequences ignoring underlying data structure. might naively construct single linear model ignores group-level variation treats observations independent. oversimplified approach fails account fact observations within groups similar due shared characteristics.\\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]can see basic linear model produced statistically significant regression analysis (t998 = 11.24, p <0.001) R^2 0.11. medium effect positive relationship changes x y (Estimate = 2.765, S.E. = 0.25).can see clearly produce simple plot x y:\nFigure 1.1: Simple scatter plot x y\nuse function geom_smooth() scatter plot, plot also includes fitted regression line obtained using \"lm\" method. allows us examine overall trend potential linear association variables.\nFigure 1.2: Scatter plot displaying relationship independent variable dependent variable. points represent observed data, fitted regression line represents linear relationship variables. plot helps visualize trend potential association variables.\ncheck_model function performance package (Lüdecke et al. (2023)) used evaluate performance diagnostic measures statistical model. provides comprehensive assessment model's fit, assumptions, predictive capabilities. calling function, can obtain summary various evaluation metrics diagnostic plots specified model.enables identify potential issues, violations assumptions, influential data points, lack fit, can affect interpretation reliability model's resultsLooking fit model tempted conclude accurate robust model.However, data hierarchically structured, individuals nested within groups, typically correlation similarity among observations within group. accounting clustering effect, estimates derived single model can biased inefficient. assumption independence among observations violated, leading incorrect standard errors inflated significance levels.figure can see difference median range x values within groups:\nFigure 1.3: Linear model conducted data\nfigure, colour tag data points group, can useful determining mixed model appropriate.:colour-coding data points based grouping variable, plot allows visually assess within-group -group variability. noticeable differences data patterns dispersion among groups, suggests data may hierarchical structure, observations within group similar observations groups.","code":"\nbasic_model <- lm(y ~ x, data = data)\nsummary(basic_model)## \n## Call:\n## lm(formula = y ~ x, data = data)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -37.23 -12.11  -2.36  11.00  44.53 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  21.0546     1.6490  12.768   <2e-16 ***\n## x             2.5782     0.2854   9.034   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.96 on 428 degrees of freedom\n## Multiple R-squared:  0.1602, Adjusted R-squared:  0.1582 \n## F-statistic: 81.62 on 1 and 428 DF,  p-value: < 2.2e-16\nplot(data$x, data$y)\nggplot(data, aes(x = x, \n                 y = y)) +\n  geom_point() +\n  labs(x = \"Independent Variable\", \n       y = \"Dependent Variable\")+\n  geom_smooth(method = \"lm\")\nperformance::check_model(basic_model)\nggplot(data, aes(x = group, \n                 y = y)) +\n  geom_boxplot() +\n  labs(x = \"Groups\", \n       y = \"Dependent Variable\")\n# Color tagged by group\nplot_group <- ggplot(data, aes(x = x, \n                               y = y, \n                               color = group,\n                               group = group)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Data Coloured by Group\", \n       x = \"Independent Variable\", \n       y = \"Dependent Variable\")+\n  theme(legend.position=\"none\")\n\nplot_group"},{"path":"foundations-of-mixed-modelling.html","id":"multiple-analyses-approach","chapter":"1 Foundations of Mixed Modelling","heading":"1.3.2 Multiple analyses approach","text":"Running separate linear models per group, also known stratified analysis, can feasible approach certain situations. However, several drawbacks includingIncreased complexityIncreased complexityInability draw direct conclusions overall variabilityInability draw direct conclusions overall variabilityReduced statistical powerReduced statistical powerInflated Type 1 error riskInflated Type 1 error riskInconsistent estimatesInconsistent estimatesLimited ability handle unbalanced/missing dataLimited ability handle unbalanced/missing data\nFigure 1.4: Scatter plot showing relationship independent variable (x) dependent variable (y) colored group. subplot represents different group. line represents group-level linear regression smoothing.\ncode , dataset data first grouped variable 'group' using group_by function, data within group nested using nest function. results new dataset nested_data group's data stored nested tibble.Next, linear regression model (lm) fit nested data group using map function. broom::tidy function applied model using map extract model summary statistics, coefficients, p-values, standard errors. resulting models stored models object.bind_rows function used combine model results single data frame called combined_models. data frame filtered include rows predictor 'x' using filter function, resulting filtered_models data frame.add column group index, rowid_to_column function applied filtered_models data frame, creating group_indexed_models data frame additional column named 'group'.Finally, p-values group_indexed_models data frame modified using custom function report_p","code":"\n# Plotting the relationship between x and y with group-level smoothing\nggplot(data, aes(x = x, y = y, color = group, group = group)) +\n  geom_point(alpha = 0.6) +  # Scatter plot of x and y with transparency\n  labs(title = \"Data Colored by Group\", x = \"Independent Variable\", y = \"Dependent Variable\") +\n  theme(legend.position = \"none\") +\n  geom_smooth(method = \"lm\") +  # Group-level linear regression smoothing\n  facet_wrap(~group)  # Faceting the plot by group\n# Creating nested data by grouping the data by 'group'\nnested_data <- data %>%\n  group_by(group) %>%\n  nest()\n\n# Fitting linear regression models to each nested data group\nmodels <- map(nested_data$data, ~ lm(y ~ x, data = .)) %>% \n          map(broom::tidy)\n\n# Combining the model results into a single data frame\ncombined_models <- bind_rows(models)\n\n# Filtering the rows to include only the 'x' predictor\nfiltered_models <- combined_models %>%\n                   filter(term == \"x\")\n\n# Adding a column for the group index using rowid_to_column function\ngroup_indexed_models <- filtered_models %>%\n                        rowid_to_column(\"group\")\n\n# Modifying the p-values using a custom function report_p\nfinal_models <- group_indexed_models %>%\n                mutate(p.value = report_p(p.value))\n\nfinal_models\n report_p <- function(p, digits = 3) {\n     reported <- if_else(p < 0.001,\n             \"p < 0.001\",\n             paste(\"p=\", round(p, digits)))\n     \n     return(reported)\n }"},{"path":"foundations-of-mixed-modelling.html","id":"complex-model","chapter":"1 Foundations of Mixed Modelling","heading":"1.3.3 Complex model","text":"Using group level term interaction x fixed effect means explicitly including interaction term x group predictor model equation. approach assumes relationship x outcome variable differs across groups differences constant fixed. implies group unique intercept (baseline level) slope (effect size) relationship x outcome variable. treating group level term fixed effect, model estimates specific parameter values group.explicitly interested outcomes differences individual group (wish account ) - may best option can lead overfitting uses lot degrees freedom - impacting estimates widening confidence intervals. running multiple models , limited ability make inferences outside observed groups, handle missing data unbalanced designs well.\\[Y_i = \\beta_0 + \\beta_1X_i + \\beta_2.group_i+\\beta_3(X_i.group_i)+\\epsilon_i\\]","code":"\nadditive_model <- lm(y ~ x*group, data = data)\n\nsummary(additive_model)## \n## Call:\n## lm(formula = y ~ x * group, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -24.8614  -6.2579   0.2044   6.9342  28.5474 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   6.8125     1.8881   3.608 0.000346 ***\n## x             3.0644     0.3379   9.070  < 2e-16 ***\n## group2        6.4526     2.7289   2.365 0.018505 *  \n## group3       44.8356     2.8539  15.710  < 2e-16 ***\n## group4       15.8607     2.7184   5.835 1.08e-08 ***\n## group5       22.9090     4.0772   5.619 3.51e-08 ***\n## x:group2     -0.5481     0.4875  -1.124 0.261560    \n## x:group3     -1.6739     0.4781  -3.501 0.000513 ***\n## x:group4     -1.3787     0.4830  -2.855 0.004522 ** \n## x:group5     -3.1400     0.7479  -4.198 3.28e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.801 on 420 degrees of freedom\n## Multiple R-squared:  0.7246, Adjusted R-squared:  0.7187 \n## F-statistic: 122.8 on 9 and 420 DF,  p-value: < 2.2e-16"},{"path":"foundations-of-mixed-modelling.html","id":"our-first-mixed-model","chapter":"1 Foundations of Mixed Modelling","heading":"1.4 Our first mixed model","text":"mixed model good choice : allow us use data (higher sample size) account correlations data coming groups. also estimate fewer parameters avoid problems multiple comparisons encounter using separate regressions.can now join random effect \\(U_j\\) full dataset define y values \\[Y_{ij} = β_0 + β_1*X_{ij} + U_j + ε_{ij}\\].response variable, attempting explain part variation test score fitting independent variable fixed effect. response variable residual variation (.e. unexplained variation) associated group. using random effects, modeling unexplained variation variance.now want know association y ~ x exists controlling variation group.","code":""},{"path":"foundations-of-mixed-modelling.html","id":"running-mixed-effects-models-with-lmertest","chapter":"1 Foundations of Mixed Modelling","heading":"1.4.1 Running mixed effects models with lmerTest","text":"section detail run mixed models lmer function R package lmerTest (Kuznetsova et al. (2020)). builds older lme4 (Bates et al. (2023)) package, particular add p-values previously included. R packages can used run mixed-effects models including nlme package (Pinheiro et al. (2023)) glmmTMB package (Brooks et al. (2023)). Outside R also packages software capable running mixed-effects models, though arguably none better supported R software.groups clearly explain lot varianceSo differences groups explain ~67% variance ’s “left ” variance explained fixed effects.","code":"\nplot_function2 <- function(model, title = \"Data Coloured by Group\"){\n  \ndata <- data %>% \n  mutate(fit.m = predict(model, re.form = NA),\n         fit.c = predict(model, re.form = NULL))\n\ndata %>%\n  ggplot(aes(x = x, y = y, col = group)) +\n  geom_point(pch = 16, alpha = 0.6) +\n  geom_line(aes(y = fit.c, col = group), linewidth = 2)  +\n  coord_cartesian(ylim = c(-40, 100))+\n  labs(title = title, \n       x = \"Independent Variable\", \n       y = \"Dependent Variable\") \n}\n\nmixed_model <- lmer(y ~ x + (1|group), data = data)\n\nplot_function2(mixed_model, \"Random intercept\")\n# random intercept model\nmixed_model <- lmer(y ~ x + (1|group), data = data)\n\nsummary(mixed_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: y ~ x + (1 | group)\n##    Data: data\n## \n## REML criterion at convergence: 3224.4\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.61968 -0.63654 -0.03584  0.66113  3.13597 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  group    (Intercept) 205      14.32   \n##  Residual             101      10.05   \n## Number of obs: 430, groups:  group, 5\n## \n## Fixed effects:\n##             Estimate Std. Error       df t value Pr(>|t|)    \n## (Intercept)  23.2692     6.4818   4.1570    3.59   0.0215 *  \n## x             2.0271     0.1703 424.0815   11.90   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##   (Intr)\n## x -0.131\n205/(205 + 101) = 0.669 / 66.9%\n"},{"path":"foundations-of-mixed-modelling.html","id":"partial-pooling","chapter":"1 Foundations of Mixed Modelling","heading":"1.4.2 Partial pooling","text":"worth noting random effect estimates function group-level information overall (grand) mean random effect. Group levels low sample size poor information (.e., strong relationship) strongly influenced grand mean, adds information otherwise poorly-estimated group. However, group large sample size strong information (.e., strong relationship) little influence grand mean largely reflect information contained entirely within group. process called partial pooling (opposed pooling, effect considered, total pooling, separate models run different groups). Partial pooling results phenomenon known shrinkage, refers group-level estimates shrunk toward mean. mean? use random effect, prepared factor levels influence overall mean levels. good, clear signal group, won’t see much impact overall mean, small groups without much signal.can take look estimates standard errors three previously constructed models:Pooling helps improve precision estimates borrowing strength entire dataset. However, can also lead differences estimates standard errors compared models without pooling.Pooled: pooled model averaged estimates may accurately reflect true values within group. result, estimates pooled models can biased towards average behavior across groups. can see small standard error intercept, underestimating variance dataset. time substantial variability relationships groups, pooled estimates can less precise. increased variability across groups can contribute larger standard errors difference (SED) fixed effects pooled models.Pooled: pooled model averaged estimates may accurately reflect true values within group. result, estimates pooled models can biased towards average behavior across groups. can see small standard error intercept, underestimating variance dataset. time substantial variability relationships groups, pooled estimates can less precise. increased variability across groups can contribute larger standard errors difference (SED) fixed effects pooled models.pooling: model extremely precise smallest errors, however estimates reflect conditions first group modelNo pooling: model extremely precise smallest errors, however estimates reflect conditions first group modelPartial pooling/Mixed models: model reflects greater uncertainty Mean SE intercept. However, SED partial pooling model accounts variability within groups uncertainty groups. Compared pooling approach, SED partial pooling model tends smaller incorporates pooled information, reduces overall uncertainty. adjusted SED provides accurate measure uncertainty associated estimated differences groups fixed effects.Partial pooling/Mixed models: model reflects greater uncertainty Mean SE intercept. However, SED partial pooling model accounts variability within groups uncertainty groups. Compared pooling approach, SED partial pooling model tends smaller incorporates pooled information, reduces overall uncertainty. adjusted SED provides accurate measure uncertainty associated estimated differences groups fixed effects.","code":"\npooled <- basic_model %>% \n  broom::tidy() %>% \n  mutate(Approach = \"Pooled\", .before = 1) %>% \n  select(term, estimate, std.error, Approach)\n\nno_pool <- additive_model %>% \n  broom::tidy() %>% \n  mutate(Approach = \"No Pooling\", .before = 1) %>% \n  select(term, estimate, std.error, Approach)\n\npartial_pool <- mixed_model %>% \n  broom.mixed::tidy() %>% \n  mutate(Approach = \"Mixed Model/Partial Pool\", .before = 1) %>% \n  select(Approach, term, estimate, std.error)\n\nbind_rows(pooled, no_pool, partial_pool) %>% \n  filter(term %in% c(\"x\" , \"(Intercept)\") )"},{"path":"foundations-of-mixed-modelling.html","id":"variance-and-model-outcomes","chapter":"1 Foundations of Mixed Modelling","heading":"1.5 Variance and model outcomes","text":"One misconception mixed-effects models produce estimates relationships group.?can use coef() function extract estimates (strictly predictions) random effects. output several components.ESTIMATES VS PREDICTIONS\nhttp://optimumsportsperformance.com/blog/making-predictions---mixed-model-using-r/\nBLUPSThis function produces 'best linear unbiased predictions' (BLUPs) intercept slope regression site. predictions given different get ran individual models site, BLUPs product compromise complete-pooling -pooling models. Now predicted intercept influenced sites leading process called 'shrinkage'.called predictions estimates? estimated variance site (bargain using single degree freedom random effect matter many levels!), essentially borrowed information across sites, improve accuracy, combine fixed effects. strictest sense predicting relationships rather just direct observation.generous ability make predictions one main advantages mixed-model.summary() function already provided estimates fixed effects, can also extracted fixef() function.can also apply anova() single model get F-test fixed effect","code":"\ncoef(mixed_model)## $group\n##   (Intercept)        x\n## 1    11.82356 2.027066\n## 2    15.68146 2.027066\n## 3    47.94678 2.027066\n## 4    21.01028 2.027066\n## 5    19.88385 2.027066\n## \n## attr(,\"class\")\n## [1] \"coef.mer\"\nfixef(mixed_model)## (Intercept)           x \n##   23.269187    2.027066\nanova(mixed_model)"},{"path":"foundations-of-mixed-modelling.html","id":"shrinkage-in-mixed-models","chapter":"1 Foundations of Mixed Modelling","heading":"1.5.1 Shrinkage in mixed models","text":"graph demonstrates compromise complete pooling pooling. plots overall regression line/mean (fixed effects lmer model), predicted slopes site mixed-effects model, compares estimates site (nested lm).can see groups show shrinkage, deviate less overall mean, obviously group 5, sample size deliberately reduced. can see predicted line much closer overall regression line, reflecting greater uncertainty. slope drawn towards overall mean shrinkage.\nFigure 1.5: Regression relationships fixed-effects mixed effects models, note shrinkage group 5\n","code":"\n# Nesting the data by group\nnested_data <- data %>% \n    group_by(group) %>% \n    nest()\n\n# Fitting linear regression models and obtaining predictions for each group\nnested_models <- map(nested_data$data, ~ lm(y ~ x, data = .)) %>% \n    map(predict)\n# Creating a new dataframe and adding predictions from different models\ndata1 <- data %>% \n  mutate(fit.m = predict(mixed_model, re.form = NA),\n         fit.c = predict(mixed_model, re.form = NULL)) %>% \n  arrange(group,obs) %>% \n  mutate(fit.l = unlist(nested_models)) \n\n# Creating a plot to visualize the predictions\ndata1 %>% \n  ggplot(aes(x = x, y = y, colour = group)) +\n    geom_point(pch = 16) + \n  geom_line(aes(y = fit.l, linetype = \"lm\"), colour = \"black\")+\n  geom_line(aes(y = fit.c, linetype = \"lmer\"))+ \n  geom_line(aes(y = fit.m, linetype = \"Mean\"), colour = \"grey\")+\n   scale_linetype_manual(name = \"Model Type\", \n                        labels = c(\"Mean\", \"lmer\", \"lm\"),\n                        values = c(\"dotdash\", \"solid\", \"dashed\"))+\n  facet_wrap( ~ group)+\n  guides(colour = \"none\")\nplot_model(mixed_model, terms = c(\"x\", \"group\"), type = \"re\")\nplot_model(mixed_model, terms = c(\"x\", \"group\"), type = \"est\")\nbasic_pred <- emmeans(basic_model, specs = ~  x, at = list(x = c(0, 2.5, 5, 7.5, 10))) %>% as_tibble()\n\nmixed_pred <- emmeans(mixed_model, specs = ~  x, at = list(x = c(0, 2.5, 5, 7.5, 10))) %>% as_tibble()\n\npooled_plot <- data %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(pch = 16, alpha = 0.6, aes(col = group)) +\n  geom_line(aes(x = x, y = emmean), linewidth = 1, data = basic_pred) +\n  geom_line(aes(x = x, y = lower.CL), linewidth = 0.5, linetype = 2, col = \"gray50\", data = basic_pred) +\n  geom_line(aes(x = x, y = upper.CL), linewidth = 0.5, linetype = 2, col = \"gray50\", data = basic_pred) +\n  coord_cartesian(ylim = c(0, 70))+\n  labs(title = \"Pooled model\",\n       x = \"Independent Variable\", \n       y = \"Dependent Variable\") +\n  theme(legend.position = \"none\")\n\npartial_plot <- data %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(pch = 16, alpha = 0.6, aes(col = group)) +\n  geom_line(aes(x = x, y = emmean), linewidth = 1, data = basic_pred) +\n  geom_line(aes(x = x, y = lower.CL), linewidth = 0.5, linetype = 2, col = \"gray50\", data = mixed_pred) +\n  geom_line(aes(x = x, y = upper.CL), linewidth = 0.5, linetype = 2, col = \"gray50\", data = mixed_pred) +\n  coord_cartesian(ylim = c(0, 70))+\n  labs(title = \"Mixed model\",\n       x = \"Independent Variable\", \n       y = \"Dependent Variable\") +\n  theme(legend.position = \"none\")\n\npooled_plot /\n  partial_plot"},{"path":"foundations-of-mixed-modelling.html","id":"ggeffects","chapter":"1 Foundations of Mixed Modelling","heading":"1.5.2 ggeffects","text":"","code":""},{"path":"foundations-of-mixed-modelling.html","id":"ggpredict","chapter":"1 Foundations of Mixed Modelling","heading":"1.5.2.1 ggpredict","text":"argument type = random ggpredict function (ggeffects package Lüdecke (2023)) used specify type predictions generated context mixed effects models. main difference using ggpredict without type = random lies type predictions produced:Without type = random: ggpredict generate fixed-effects predictions. predictions based solely fixed effects model, ignoring variability associated random effects. resulting predictions represent average expected values response variable specific combinations predictor values, considering fixed components model.type = random: ggpredict generate predictions incorporate fixed random effects. predictions take account variability introduced random effects model. resulting predictions reflect average trend captured fixed effects also additional variability associated random effects different levels grouping factor(s).","code":""},{"path":"foundations-of-mixed-modelling.html","id":"ggemmeans","chapter":"1 Foundations of Mixed Modelling","heading":"1.5.2.2 ggemmeans","text":"","code":""},{"path":"foundations-of-mixed-modelling.html","id":"sjplot","chapter":"1 Foundations of Mixed Modelling","heading":"1.5.3 sjPlot","text":"BLUP - Hector book!","code":"\nplot_model(mixed_model,type=\"pred\",\n           terms=c(\"x\", \"group\"),\n           pred.type=\"re\",\n           show.data = T)+\n  facet_wrap( ~ group)"},{"path":"foundations-of-mixed-modelling.html","id":"checking-model-assumptions","chapter":"1 Foundations of Mixed Modelling","heading":"1.6 Checking model assumptions","text":"\nFigure 1.6: Marginal fit, heavy black line random effect model histogram distribution conditional intercepts\nre.form = NA: re.form set NA, indicates random effects ignored prediction. means prediction based solely fixed effects model, ignoring variation introduced random effects. useful interested estimating overall trend relationship described fixed effects, without considering specific random effects individual groups levels.re.form = NULL: Conversely, re.form set NULL, indicates random effects included prediction. means prediction take account fixed effects random effects associated levels random effect variable. model use estimated random effects generate predictions account variation introduced random effects. useful want visualize analyze variation response variable explained different levels random effect.\nlot data, even minimal deviations \nexpected distribution become significant (discussed \nhelp/vignette DHARMa package). need assess \ndistribution decide important .\n","code":"\nplot(mixed_model) \nqqnorm(resid(mixed_model))\nqqline(resid(mixed_model)) \nrand_dist <- as.data.frame(ranef(mixed_model)) %>% \n  mutate(group = grp,\n         b0_hat = condval,\n         intercept_cond = b0_hat + summary(mixed_model)$coef[1,1],\n         .keep = \"none\")\n\nhist(rand_dist$b0_hat)\ndata1 %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(pch = 16, col = \"grey\") +\n  geom_violinhalf(data = rand_dist, \n                  aes(x = 0, y = intercept_cond), \n                  trim = FALSE, \n                  width = 4, \n                  adjust = 2, \n                  fill = NA)+\n  geom_line(aes(y = fit.m), linewidth = 2)  +\n  coord_cartesian(ylim = c(-40, 100))+\n  labs(x = \"Independent Variable\", \n       y = \"Dependent Variable\")\nperformance::check_model(mixed_model)\nresid.mm <- DHARMa::simulateResiduals(mixed_model)\n\nplot(resid.mm)"},{"path":"foundations-of-mixed-modelling.html","id":"practice-questions","chapter":"1 Foundations of Mixed Modelling","heading":"1.7 Practice Questions","text":"","code":""},{"path":"types-of-random-effects.html","id":"types-of-random-effects","chapter":"2 Types of Random Effects","heading":"2 Types of Random Effects","text":"","code":""},{"path":"types-of-random-effects.html","id":"is-it-a-fixed-or-random-effect","chapter":"2 Types of Random Effects","heading":"2.1 Is it a fixed or random effect?","text":"BAKER BOOKMISSING DATA BAKER BOOK","code":""},{"path":"types-of-random-effects.html","id":"crossed-or-nested","chapter":"2 Types of Random Effects","heading":"2.2 Crossed or Nested","text":"https://www.muscardinus./2017/07/lme4-random-effects/common issue causes confusion issue specifying random effects either ‘crossed’ ‘nested’. reality, way specify random effects determined experimental sampling design ( Schielzeth & Nakagawa, 2013 ). simple example can illustrate difference. Imagine researcher interested understanding factors affecting clutch mass passerine bird. study population spread across five separate woodlands, containing 30 nest boxes. Every week breeding measure foraging rate females feeders, measure subsequent clutch mass. females multiple clutches season contribute multiple data points., female ID said nested within woodland : woodland contains multiple females unique woodland (never move among woodlands). nested random effect controls fact () clutches female independent, (ii) females woodland may clutch masses similar one another females woodlandsClutch Mass ∼ Foraging Rate + (1|Woodland/Female ID)Now imagine long-term study, researcher returns every year five years continue measurements. appropriate fit year crossed random effect every woodland appears multiple times every year dataset, females survive one year next also appear multiple years.Clutch Mass ∼ Foraging Rate + (1|Woodland/Female ID)+ (1|Year)Understanding whether experimental/sampling design calls nested crossed random effects always straightforward, can help visualise experimental design drawing (see Schielzeth & Nakagawa, 2013 ; Fig. 1 ), tabulating observations grouping factors (e.g. ‘ table’ command R) identify data distributed. advocate researchers always ensure levels random effect grouping variables uniquely labelled. example, females labelled 1 - n woodland, model try pool variance females code. Giving females unique code makes nested structure data implicit, model specified ∼ (1| Woodland) + (1|FemaleID) identical model .\nFigure 2.1: Fully Nested\n\nFigure 2.2: Fully Crossed\n\nFigure 2.3: Partially Nested/Crossed\n","code":""},{"path":"types-of-random-effects.html","id":"random-slopes","chapter":"2 Types of Random Effects","heading":"2.3 Random slopes","text":"","code":"\nsummary(lmer3)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: y ~ x + (x | group)\n##    Data: data\n## \n## REML criterion at convergence: 3211\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.5477 -0.6567  0.0042  0.7018  2.9428 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr \n##  group    (Intercept) 294.2242 17.1530       \n##           x             0.9398  0.9694  -0.65\n##  Residual              96.2248  9.8094       \n## Number of obs: 430, groups:  group, 5\n## \n## Fixed effects:\n##             Estimate Std. Error      df t value Pr(>|t|)  \n## (Intercept)  24.3285     7.7423  4.0342   3.142   0.0344 *\n## x             1.8297     0.4708  3.2059   3.887   0.0268 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##   (Intr)\n## x -0.641\n## optimizer (nloptwrap) convergence code: 0 (OK)\n## Model failed to converge with max|grad| = 0.00241 (tol = 0.002, component 1)\ndata.2 <- data %>% \n  mutate(fit2.m = predict(lmer3, re.form = NA),\n         fit2.c = predict(lmer3, re.form = NULL),\n         resid2 = resid(lmer3))"},{"path":"types-of-random-effects.html","id":"random-effect-correlation","chapter":"2 Types of Random Effects","heading":"2.4 Random effect correlation","text":"Understanding Group Structure: correlation random effects provides insight inherent structure groups data. reveals degree similarity dissimilarity random effects associated different groups. information helps uncover patterns clustering similarity among groups, can crucial understanding underlying mechanisms processes studied.Modeling Assumptions: correlation random effects directly linked assumptions made covariance structure random effects model. reporting correlation, researchers can assess validity modeling assumptions. Violations deviations assumed correlation structure may indicate need alternative modeling approaches investigation nature group-level variability.","code":"\nrand_dist2 <- as.data.frame(ranef(lmer3)) %>% \n  mutate(group = grp,\n          term = case_when(term == \"(Intercept)\" ~ \"b0_hat\",\n                             term == \"x\" ~ \"b1_hat\"),\n            value = condval,\n         .keep = \"none\") %>%\n  pivot_wider(id_cols = \"group\", names_from = \"term\", values_from = \"value\") %>%\n  mutate(Intercept_cond = b0_hat + summary(lmer3)$coef[1,1],\n         Slope_cond = b1_hat + summary(lmer3)$coef[2,1])\n\npmain <- rand_dist2 %>%\n  ggplot(aes(x = Intercept_cond, y = Slope_cond)) +\n    geom_point(aes(col = group), size = 3) +\n    geom_density2d(bins = 4, col = \"grey\", adjust = 3)+\n  theme(legend.position = \"none\")\n\nxdens <- ggplot()+\n  geom_density(data = rand_dist2, aes(x = Intercept_cond), fill = \"grey\", col = NA, trim = FALSE, adjust = 2) +\n  theme_void()\n\nydens <- ggplot()+\n  geom_density(data = rand_dist2, aes(x = Slope_cond), fill = \"grey\", col = NA, trim = FALSE, adjust = 2) +\n  coord_flip()+\n  theme_void()\n\nlayout <- \"\nAAA#\nBBBC\nBBBC\nBBBC\"\n\n\ninset <- xdens+pmain+ydens +plot_layout(design = layout)\n\ninset\n((plot_function(lmer1, \"Random intercept\")+theme_classic())+coord_cartesian(\n                    ylim = c(0, 120))) + inset_element(inset, 0.1, 0.6, 0.5, 1)"},{"path":"types-of-random-effects.html","id":"model-refining","chapter":"2 Types of Random Effects","heading":"2.5 Model refining","text":"SEE !","code":""},{"path":"complex-designs.html","id":"complex-designs","chapter":"3 Complex designs","heading":"3 Complex designs","text":"1 | Site:Mix specification, random effect specified two-way crossed random effect. means random effect varies independently combination levels Site Mix factors. : operator represents interaction crossed effect two factors. specification allows correlations random effects within combination Site Mix.hand, 1 | Site/Mix specification represents nested random effect structure. suggests random effect Mix nested within random effect Site. specification assumes levels Mix nested within level Site, meaning level Site set random effects levels Mix. / operator denotes nesting relationship.summarize:1 | Site:Mix: Two-way crossed random effect, allowing independent variation random effects combination levels Site Mix.\n1 | Site/Mix: Nested random effect, random effect Mix nested within random effect Site, assuming levels Mix nested within level Site.","code":"\nbiodepth <- biodepth %>% \n  mutate(Mix = factor(Mix),\n         Diversity2 = log(Diversity, 2)) %>% \n  drop_na()\n\n# set Mix as a factor\n# Set Diversity to log, base 2\nbio.lmer1 <- lmer(Shoot2 ~ Diversity2 + (1|Site) + (1|Block) + (1|Mix), data = biodepth)\n\nsummary(bio.lmer1)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: Shoot2 ~ Diversity2 + (1 | Site) + (1 | Block) + (1 | Mix)\n##    Data: biodepth\n## \n## REML criterion at convergence: 6082.6\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.2187 -0.5179 -0.1031  0.3941  3.1735 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  Mix      (Intercept) 33665.3  183.48  \n##  Block    (Intercept)   383.2   19.57  \n##  Site     (Intercept) 30163.9  173.68  \n##  Residual             22039.8  148.46  \n## Number of obs: 451, groups:  Mix, 192; Block, 15; Site, 8\n## \n## Fixed effects:\n##             Estimate Std. Error      df t value Pr(>|t|)    \n## (Intercept)  349.488     66.244   8.576   5.276 0.000598 ***\n## Diversity2    78.901     12.059 175.159   6.543 6.42e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##            (Intr)\n## Diversity2 -0.286\nbio.lmer2 <- lmer(Shoot2 ~ Diversity2 + (Diversity2|Site) + (1|Block) + (1|Mix), data = biodepth)\n\nsummary(bio.lmer2)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: Shoot2 ~ Diversity2 + (Diversity2 | Site) + (1 | Block) + (1 |  \n##     Mix)\n##    Data: biodepth\n## \n## REML criterion at convergence: 6072.6\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.1292 -0.5208 -0.1004  0.3739  3.2865 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr\n##  Mix      (Intercept) 31421.7  177.26       \n##  Block    (Intercept)   391.6   19.79       \n##  Site     (Intercept) 18930.9  137.59       \n##           Diversity2   1149.9   33.91   1.00\n##  Residual             21995.1  148.31       \n## Number of obs: 451, groups:  Mix, 192; Block, 15; Site, 8\n## \n## Fixed effects:\n##             Estimate Std. Error      df t value Pr(>|t|)    \n## (Intercept)  346.070     54.362   8.734   6.366 0.000149 ***\n## Diversity2    79.181     16.855  11.289   4.698 0.000608 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##            (Intr)\n## Diversity2 0.399 \n## optimizer (nloptwrap) convergence code: 0 (OK)\n## boundary (singular) fit: see help('isSingular')"},{"path":"complex-designs.html","id":"likelihood-ratio-tests","chapter":"3 Complex designs","heading":"3.1 Likelihood Ratio tests","text":"produce initial model random effects structure, may wish perform model selection comparing different nested models varying random effects structures. allows us assess whether inclusion additional random effects changes random effects structure significantly improve model fit. comparing likelihood values different nested models, can determine model provides better fit data.\nliterature idea model selection, , \nautomated (sometimes manual) way testing many versions model\ndifferent subset predictors attempt find \nmodel fits best. sometimes called “stepwise”\nprocedures.\n\nOthers argue method number flaws, including:\n\n\nbasically “p-value mining”, , running lot\ntests till find p-value like.\n\n\nbasically “p-value mining”, , running lot\ntests till find p-value like.\n\n\nlikelihood making false positive high.\n\n\nlikelihood making false positive high.\n\n\nAdding/removing new variable can effect \npredictors.\n\n\nAdding/removing new variable can effect \npredictors.\n\nInstead model selection, use knowledge \ndata select subset variables either ) \nimportance , b) theoretically influential outcome. \ncan fit single model including .\n\nHowever, argue inclusion random effects \nstructure clear rationale implementation,\nadjustments better understand best type random effects\nstructure perfectly reasonable.\ncan perform Likelihood ratio test (LRT) anova() function:Frequentist fit used LMM lme4 / lmer based Maximum Likelihood principle, maximize likelihood \\(L(y)\\) observing data \\(y\\), equivalent minimizing residuals model, Ordinary Least Squares (OLS) approach. measures probability observing data given specific set parameter values.attempting optimise model can use likelihood ratio test (LRT). Given two nested models, denoted Model 1 Model 2, LRT compares likelihood values models assess whether complex Model 2 provides significantly better fit data compared simpler Model 1. LRT statistic, denoted \\(D\\), calculated difference log-likelihood values Model 1 Model 2, multiplied 2:\\[D = -2~*~(ln(L_1)-ln(L_2))\\]\\(L_1\\) represents likelihood value Model 1, \\(L_2\\) represents likelihood value Model 2. LRT statistic follows chi-square (\\(\\chi^2\\)) distribution degrees freedom equal difference number parameters two models.determine statistical significance LRT statistic, one can compare critical value chi-square distribution appropriate degrees freedom. LRT statistic exceeds critical value, indicates complex Model 2 provides significantly better fit data compared simpler Model 1.ML estimation often used perform hypothesis tests, including chi-square test. chi-square test compares observed data expected data predicted statistical model. assesses goodness--fit observed data model's predictions.","code":"\nbio.lmer3 <- lmer(Shoot2 ~ Diversity2 + (Diversity2|Site) + (1|Mix), data = biodepth)\n\nanova(bio.lmer2, bio.lmer3)"},{"path":"complex-designs.html","id":"reml","chapter":"3 Complex designs","heading":"3.1.1 REML","text":"REML (Restricted Maximum Likelihood) estimation variant ML estimation addresses issue bias estimation random effects mixed effects models. mixed effects models, random effects account variation group individual level explained fixed effects. However, inclusion random effects introduces bias ML estimates, influenced variability random effects.REML estimation addresses bias optimizing likelihood function conditional fixed effects , effectively removing influence random effects estimation. approach provides unbiased estimates fixed effects especially useful primary interest lies fixed effects rather random effects.","code":""},{"path":"complex-designs.html","id":"ml-vs.-reml-fitting","chapter":"3 Complex designs","heading":"3.1.2 ML vs. REML fitting","text":"Maximum Likelihood (ML) estimation preferable comparing nested models allows direct comparison likelihood values different models. ML estimation provides quantitative measure well given model fits observed data, based likelihood function.context, ML estimation preferable allows formal statistical comparison nested models. provides rigorous objective way assess whether inclusion additional parameters complex model leads significantly better fit data compared simpler model. approach ensures model comparisons based sound statistical principles helps determining appropriate model given data.Older versions model fitting packages like lmer used require manual switch REML ML fitting models order switch objectives assessing goodness--fit interpreting estimates. notice perform LRT anova() function informs switch made automatically.","code":""},{"path":"complex-designs.html","id":"model-predictions","chapter":"3 Complex designs","heading":"3.1.3 Model predictions","text":"","code":"\nnested_data <- biodepth %>% \n    group_by(Site) %>% \n    nest()\n\nmodels <- map(nested_data$data, ~ lm(Shoot2 ~ Diversity2, data = .)) %>% \n    map(predict)\nbiodepth.2 <- biodepth %>% \n    mutate(fit.m = predict(bio.lmer2, re.form = NA),\n           fit.c = predict(bio.lmer2, re.form = ~(1+Diversity2|Site)),\n           fit.l = unlist(models))\n\nbiodepth.2 %>% \n    ggplot(aes(x = Diversity2, y = Shoot2, colour = Site)) +\n    geom_point(pch = 16) + \n    geom_line(aes(y = fit.l), color = \"black\")+\n    geom_line(aes(y = fit.c))+ \n    geom_line(aes(y = fit.m), colour = \"grey\",\n              linetype = \"dashed\")+\n    facet_wrap( ~ Site)+\n   coord_cartesian(ylim = c(0, 1200))+\n   theme(legend.position = \"none\")"},{"path":"reporting-mixed-model-results.html","id":"reporting-mixed-model-results","chapter":"4 Reporting Mixed Model results","heading":"4 Reporting Mixed Model results","text":"BAKER - anova() ranova() MuMIn r.squaredGLMM","code":""},{"path":"reporting-mixed-model-results.html","id":"tables","chapter":"4 Reporting Mixed Model results","heading":"4.1 Tables","text":"","code":""},{"path":"reporting-mixed-model-results.html","id":"figures","chapter":"4 Reporting Mixed Model results","heading":"4.2 Figures","text":"","code":""},{"path":"reporting-mixed-model-results.html","id":"write-ups","chapter":"4 Reporting Mixed Model results","heading":"4.3 Write-ups","text":"","code":""},{"path":"worked-example-1.html","id":"worked-example-1","chapter":"5 Worked Example 1","heading":"5 Worked Example 1","text":"https://bodowinter.com/tutorial/bw_LME_tutorial.pdf","code":"\npoliteness <- read_csv(\"files/politeness_data.csv\")\n\npoliteness.model.1 = lmer(frequency ~ gender + attitude + (1+attitude|subject) +\n                            (1+attitude|scenario),\n                        data=politeness)"},{"path":"worked-example-2---dolphins.html","id":"worked-example-2---dolphins","chapter":"6 Worked Example 2 - Dolphins","heading":"6 Worked Example 2 - Dolphins","text":"DOLPHIN FIXED EFFECTS","code":"\ndolphmod.1 <- lmer(vt ~ bodymass + direction + (direction|animal), data=dolphins)\ndolphmod.2 <- lmer(vt ~ bodymass + direction + (1|animal), data=dolphins)\ndolphins.1 <- dolphins %>% \n    mutate(fit.m = predict(dolphmod.2, re.form = NA),\n           fit.c = predict(dolphmod.2, re.form = NULL))\ndolphins.1 %>%\n  ggplot(aes(x = bodymass, y = vt, group = direction)) +\n  geom_point(pch = 16, aes(colour = animal)) +\n  geom_line(aes(y = fit.m, \n                linetype = direction), \n            linewidth = 1)  +\n  labs(x = \"Body Mass\", \n       y = \"VT\") \nplot_model(dolphmod.2,type=\"pred\",\n           terms=c(\"bodymass\", \"direction\"),\n           pred.type=\"fe\",\n           show.data = T)\nplot_model(dolphmod.2,type=\"pred\",\n           terms=c(\"bodymass\", \"direction\", \"animal\"),\n           pred.type=\"re\",\n           show.data = T)"},{"path":"summary.html","id":"summary","chapter":"7 Summary","heading":"7 Summary","text":"suggested workflow:Start model containing fixed effects: Begin fitting model fixed effects relevant research question. Include main predictors potential interactions hypothesize might exist. example, predictors B, might start model like response ~ + B + :B.Assess fixed effects interactions: Evaluate significance, direction, magnitude fixed effects coefficients. Look interactions show significant effects consider interpretation context research question. step allows identify key variables interactions important explaining variation response variable.Model evaluation refinement: Assess goodness fit fixed effects model using appropriate measures like AIC, BIC, model deviance. Consider conducting model comparison evaluate different models alternative fixed effects structures. process helps refine model select appropriate combination variables interactions.Incorporate random effects: identified significant fixed effects relevant interactions, can consider inclusion random effects. Random effects capture variation different levels can account individual differences clustering within groups. Evaluate need random intercepts, random slopes, crossed random effects based research design nature data.Assess compare models random effects: Fit models random effects compare fit fixed effects model. Consider appropriate measures likelihood ratio tests, AIC, BIC model comparison. Evaluate significance contribution random effects model.Validate interpret final model: Validate final model assessing assumptions, checking influential observations, performing sensitivity analysis. Interpret estimated coefficients, including fixed effects random effects, context research question. Report results, including confidence intervals p-values.initially focusing fixed effects, can establish foundation model identify significant predictors interactions. step allows better understand relationships data guide subsequent inclusion random effects appropriate.https://ademos.people.uic.edu/Chapter17.html#121_crossed__nested_designs","code":""},{"path":"summary.html","id":"mixed-model-extensions","chapter":"7 Summary","heading":"7.1 Mixed Model extensions","text":"","code":""},{"path":"summary.html","id":"practical-problems","chapter":"7 Summary","heading":"7.2 Practical problems","text":"HARRISON?MODEL CONVERGENCE - BAKERBOUNDARY FITlmer3 <- lmer(y ~ x + (x | group), data = data, control = lmerControl(optimizer =\"Nelder_Mead\"))CHECK !mixed models, solution arrived iteratively, means can fail converge number reasons. Generally, failure converge due issue data.Linear model analyses can extend beyond testing differences means categorical groupings test relationships continuous variables. known linear regression, relationship explanatory variable response variable modelled equation straight line. intercept value y x = 0, often useful, can use 'mean-centered' values wish make intercept intuitive.\nlinear models, regression assumes unexplained variability around regression line, normally distributed constant variance.regression fitted possible predict values y values x, uncertainty around predictions can captured confidence intervals.first thing try scaling parameters:egen scaled_var = std(var)\nDummy variables typically don’t need scaled, can . scaling variables allows convergence, try different combinations scaled unscaled figure variables causing problem. ’s typically (always) variables largest scale begin .Another potential convergence issue extremely high correlation predictors (including dummy variables). already addressed considering multicollinearity, , can make convergence challenging.iteration keeps running (opposed ending complaining lack convergence), try passing option iterate(#) “large” (“large” relative running time) numbers tell algorithm stop # iterations, regardless convergence. (Recall iterative solution produces answer iteration, ’s just consistent answer reach convergence.) ’re looking two things:First, estimated standard errors extremely close zero exploding towards infinity, predictor may causing issue. Try removing .\nSecond, try different max iterations (say 50, 100 200), estimated coefficients standard errors relatively constant, may running case model converging just beyond tolerance Stata uses, intents purposes converged. Set iterate(#) high number use result.\ncan try use “reml” optimizer, passing reml option. optimizer can bit easier converge, though may slower.Finally, although pains admit , try running model different software R SAS. software slightly different algorithms uses, situations one software converge another won’t","code":""},{"path":"summary.html","id":"further-reading","chapter":"7 Summary","heading":"7.3 Further Reading","text":"suggested starter topics continue mixed-model journey!brief introduction mixed effects modelling multi-model inference ecology Harrison et al. (2018)brief introduction mixed effects modelling multi-model inference ecology Harrison et al. (2018)Mixed Effects Models Extensions Ecology R Zuur et al. (2009)Mixed Effects Models Extensions Ecology R Zuur et al. (2009)Perils pitfalls mixed-effects regression models biologySilk et al. (2020)Perils pitfalls mixed-effects regression models biologySilk et al. (2020)","code":""}]
